{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "from utils.data_utils import *\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "\n",
    "from keras.models import Input, Model, Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_windows():\n",
    "    return sys.platform == 'win32'\n",
    "\n",
    "res_dir = os.path.join('predictions')\n",
    "if is_windows():\n",
    "    l_dirs = ['0.21', '0.22', '0.23', '0.24', '0.25']\n",
    "else:\n",
    "    l_dirs = ['0.09']\n",
    "    \n",
    "def get_data_dir(a, t):\n",
    "    if t:\n",
    "        if a:\n",
    "            data_dir = 'train_ex'\n",
    "        else:\n",
    "            data_dir = 'train'\n",
    "    else:\n",
    "        if a:\n",
    "            data_dir = 'valid_ex'\n",
    "        else:\n",
    "            data_dir = 'valid'\n",
    "    return data_dir\n",
    "\n",
    "def load_loss_df(l_dirs, augmented=False, train=False):\n",
    "    data_dir = get_data_dir(augmented, train)\n",
    "    dataframes = []\n",
    "    for d in l_dirs:\n",
    "        dataframes.append(pd.read_csv(os.path.join(res_dir, data_dir, d, 'models.csv')))\n",
    "\n",
    "    df_l = pd.concat(dataframes, ignore_index=True)\n",
    "    return df_l\n",
    "\n",
    "def get_gt_name(a, t):\n",
    "    if t:\n",
    "        if a:\n",
    "            name = 'Y_train_augmented.npz'\n",
    "        else:\n",
    "            name = 'Y_train.npz'\n",
    "    else:\n",
    "        if a:\n",
    "            name = 'Y_valid_augmented.npz'\n",
    "        else:\n",
    "            name = 'Y_valid.npz'\n",
    "    return name\n",
    "\n",
    "def load_ground_truth(data_dir, augmented=False, train=False):\n",
    "    gt_name = get_gt_name(augmented, train)\n",
    "    y_true_path = os.path.join('augmented', data_dir, gt_name)\n",
    "    y_true = npz_to_ndarray(np.load(y_true_path))\n",
    "    return y_true\n",
    "\n",
    "def load_preds(df_loss, transposed=False):\n",
    "    preds = []\n",
    "    for npz_path in df_loss.res_path:\n",
    "        p = npz_to_ndarray(np.load(npz_path))\n",
    "        if transposed:\n",
    "            preds.append(p.transpose())\n",
    "        else:\n",
    "            preds.append(p)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_augmented = True\n",
    "valid_augmented = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "df_loss_valid = load_loss_df(l_dirs, augmented=valid_augmented, train=False)\n",
    "print(len(df_loss_valid))\n",
    "df_loss_train = load_loss_df(l_dirs, augmented=train_augmented, train=True)\n",
    "print(len(df_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 17, 64768)\n",
      "(76, 17, 259064)\n"
     ]
    }
   ],
   "source": [
    "preds_valid = load_preds(df_loss_valid, transposed=True)\n",
    "print(preds_valid.shape)\n",
    "preds_train = load_preds(df_loss_train, transposed=True)\n",
    "print(preds_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64768, 17)\n",
      "(259064, 17)\n"
     ]
    }
   ],
   "source": [
    "y_true_valid = load_ground_truth('train_augmented_size_128_mult_8_seed_0', augmented=valid_augmented, train=False)\n",
    "print(y_true_valid.shape)\n",
    "y_true_train = load_ground_truth('train_augmented_size_128_mult_8_seed_0', augmented=train_augmented, train=True)\n",
    "print(y_true_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ensemble_avg_score(ensemble, all_preds, y_true, opt_th=False, opt_step=0.05, th=0.2):\n",
    "    ensemble_preds = [all_preds[j] for j in ensemble]\n",
    "    avg_pred = np.mean(ensemble_preds, axis=0)\n",
    "    if not opt_th:\n",
    "        avg_pred_final = (avg_pred > th).astype(int)\n",
    "    else:\n",
    "        thresholds = find_ratios(y_true, avg_pred, step=opt_step)\n",
    "        avg_pred_final = (avg_pred > thresholds).astype(int)\n",
    "    avg_score = fbeta_score(y_true, avg_pred_final, beta=2, average='samples')\n",
    "    return avg_score\n",
    "\n",
    "def opt_thresholds(ensemble, all_preds, y_true, opt_step=0.05):\n",
    "    ensemble_preds = [all_preds[j] for j in ensemble]\n",
    "    avg_pred = np.mean(ensemble_preds, axis=0)\n",
    "    thresholds = find_ratios(y_true, avg_pred, step=opt_step)\n",
    "    avg_pred_final = (avg_pred > thresholds).astype(int)\n",
    "    avg_score = fbeta_score(y_true, avg_pred_final, beta=2, average='samples')\n",
    "    return np.array(thresholds), avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.92870 of models [20, 62, 13, 46, 8]\n"
     ]
    }
   ],
   "source": [
    "# ensemble = [20, 62]\n",
    "ensemble = [20, 62, 13, 46, 8]\n",
    "# ensemble = [20, 62, 13, 46, 8, 22, 12, 51, 25]\n",
    "print(\"Score %.5f of models %s\" % (get_ensemble_avg_score(ensemble, preds_valid, y_true_valid), ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 17, 259064), (5, 17, 64768)\n"
     ]
    }
   ],
   "source": [
    "preds_train_ensemble = np.array([preds_train[j] for j in ensemble])\n",
    "preds_valid_ensemble = np.array([preds_valid[j] for j in ensemble])\n",
    "print(\"%s, %s\" % (preds_train_ensemble.shape, preds_valid_ensemble.shape))\n",
    "dim_batch_train = preds_train_ensemble.shape[2]\n",
    "dim_batch_valid = preds_valid_ensemble.shape[2]\n",
    "dim_classes = preds_train_ensemble.shape[1]\n",
    "dim_models = preds_train_ensemble.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259064, 85), (259064, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train = preds_train_ensemble.transpose().reshape((dim_batch_train, dim_classes*dim_models))\n",
    "Y_train = y_true_train\n",
    "print(\"%s, %s\" % (X_train.shape, Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64768, 85), (64768, 17)\n"
     ]
    }
   ],
   "source": [
    "X_valid = preds_valid_ensemble.transpose().reshape((dim_batch_valid, dim_classes*dim_models))\n",
    "Y_valid = y_true_valid\n",
    "print(\"%s, %s\" % (X_valid.shape, Y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.concatenate([X_train, X_valid])\n",
    "Y=np.concatenate([Y_train, Y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t, X_v, Y_t, Y_v = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 85)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 17)                1462      \n",
      "=================================================================\n",
      "Total params: 1,462\n",
      "Trainable params: 1,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(batch_shape=(None, dim_classes*dim_models))\n",
    "x = Dense(17, activation='sigmoid')(inp)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 259065 samples, validate on 64767 samples\n",
      "Epoch 1/100\n",
      "5s - loss: 0.1432 - acc: 0.9593 - val_loss: 0.0826 - val_acc: 0.9725\n",
      "Epoch 2/100\n",
      "4s - loss: 0.0783 - acc: 0.9730 - val_loss: 0.0756 - val_acc: 0.9733\n",
      "Epoch 3/100\n",
      "4s - loss: 0.0749 - acc: 0.9735 - val_loss: 0.0740 - val_acc: 0.9736\n",
      "Epoch 4/100\n",
      "4s - loss: 0.0737 - acc: 0.9737 - val_loss: 0.0732 - val_acc: 0.9738\n",
      "Epoch 5/100\n",
      "4s - loss: 0.0731 - acc: 0.9739 - val_loss: 0.0727 - val_acc: 0.9739\n",
      "Epoch 6/100\n",
      "4s - loss: 0.0726 - acc: 0.9740 - val_loss: 0.0723 - val_acc: 0.9740\n",
      "Epoch 7/100\n",
      "4s - loss: 0.0723 - acc: 0.9740 - val_loss: 0.0720 - val_acc: 0.9741\n",
      "Epoch 8/100\n",
      "4s - loss: 0.0721 - acc: 0.9741 - val_loss: 0.0719 - val_acc: 0.9741\n",
      "Epoch 9/100\n",
      "4s - loss: 0.0719 - acc: 0.9742 - val_loss: 0.0717 - val_acc: 0.9742\n",
      "Epoch 10/100\n",
      "4s - loss: 0.0718 - acc: 0.9742 - val_loss: 0.0717 - val_acc: 0.9743\n",
      "Epoch 11/100\n",
      "4s - loss: 0.0717 - acc: 0.9742 - val_loss: 0.0715 - val_acc: 0.9743\n",
      "Epoch 12/100\n",
      "4s - loss: 0.0716 - acc: 0.9743 - val_loss: 0.0714 - val_acc: 0.9743\n",
      "Epoch 13/100\n",
      "4s - loss: 0.0715 - acc: 0.9743 - val_loss: 0.0714 - val_acc: 0.9742\n",
      "Epoch 14/100\n",
      "4s - loss: 0.0714 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9743\n",
      "Epoch 15/100\n",
      "4s - loss: 0.0714 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9743\n",
      "Epoch 16/100\n",
      "4s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0712 - val_acc: 0.9743\n",
      "Epoch 17/100\n",
      "4s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0712 - val_acc: 0.9743\n",
      "Epoch 18/100\n",
      "4s - loss: 0.0712 - acc: 0.9743 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "4s - loss: 0.0712 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "4s - loss: 0.0712 - acc: 0.9743 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9743\n",
      "Epoch 22/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9743\n",
      "Epoch 25/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 36/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 37/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 39/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 44/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 46/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 47/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 48/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 51/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 52/100\n",
      "4s - loss: 0.0707 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 55/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 57/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 59/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 61/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 62/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 63/100\n",
      "4s - loss: 0.0707 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 70/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 72/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 74/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 76/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 77/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 78/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 80/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 81/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 82/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 83/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 84/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 85/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 86/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 87/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 90/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 91/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 92/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 93/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 94/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 95/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 96/100\n",
      "4s - loss: 0.0706 - acc: 0.9745 - val_loss: 0.0705 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "4s - loss: 0.0706 - acc: 0.9744 - val_loss: 0.0705 - val_acc: 0.9745\n",
      "Epoch 98/100\n",
      "4s - loss: 0.0705 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 99/100\n",
      "4s - loss: 0.0705 - acc: 0.9744 - val_loss: 0.0706 - val_acc: 0.9745\n",
      "Epoch 100/100\n",
      "4s - loss: 0.0705 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdeb80afd10>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAD8CAYAAADQSqd1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdeb85299d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "history_callback = model.fit(X_t, Y_t, batch_size=batch_size, epochs=epochs, verbose=2, \\\n",
    "          validation_data=(X_v, Y_v))\n",
    "\n",
    "# print(\"Max score %f at epoch %d\" % (np.max(scores), np.argmax(scores)))\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_callback.history['loss'])\n",
    "plt.plot(history_callback.history['val_loss'])\n",
    "plt.legend(labels=['loss', 'val_loss'])\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(scores)\n",
    "# plt.legend(labels=['val_fbeta'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94125291328732319"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_v_final = (model.predict(X_v) > 0.2).astype(int)\n",
    "fbeta_score(Y_v, pred_v_final, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94506739202288514"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_final = (model.predict(X_train) > 0.2).astype(int)\n",
    "fbeta_score(y_true_train, pred_train_final, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15.0847   -4.06113  -7.57677 -16.78234  -5.39819]\n",
      "[-0.62543  0.80531 -0.75107 -1.19027  0.14674]\n",
      "[ -8.88086  -2.84881   7.07265  36.54503  26.58259]\n",
      "[ 0.44704  0.2467   5.15314  2.4847   1.84196]\n",
      "[-0.17713 -0.99379  1.01715 -0.82558 -1.23105]\n",
      "[-2.97537  3.47419 -0.87784 -0.65745 -0.46761]\n",
      "[-0.19913  0.09611 -0.44161 -0.03633 -0.81241]\n",
      "[-1.39572 -0.43365 -1.2265  -0.56022  0.01494]\n",
      "[  2.52688   4.24145  -1.02049  11.44044   0.12588]\n",
      "[-0.99351 -0.18186 -1.105    0.03765 -0.52204]\n",
      "[-0.91355  1.76397 -3.20688 -2.0808   2.21245]\n",
      "[-0.51412 -0.58624  0.27673 -1.12249  0.27859]\n",
      "[ 2.44586 -1.38619 -0.23896 -1.23565  0.95288]\n",
      "[-11.06025  -7.47486   2.16442   8.16653   0.74888]\n",
      "[-0.66162 -0.31268 -1.07015 -1.14211 -0.1778 ]\n",
      "[-0.32547 -0.33468  0.76427 -0.29174  0.3493 ]\n",
      "[  2.91964   8.06445  -5.17206  37.90324  22.07038]\n"
     ]
    }
   ],
   "source": [
    "cls=3\n",
    "w1=model.get_weights()[0][:,cls].reshape((17,5))\n",
    "for i in range(17):\n",
    "    print(w1[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(X_r):\n",
    "    X_r_reconstructed = np.empty((5,X_r.shape[0],17))\n",
    "    temp1 = X_r.reshape((X_r.shape[0],17,5)).transpose()\n",
    "    for i in range(temp1.shape[0]):\n",
    "        X_r_reconstructed[i] = temp1[i].transpose()\n",
    "    return X_r_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94173161452971843"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_v_reconstructed = (np.mean(reconstruct(X_v), axis=0) > 0.2).astype(int)\n",
    "fbeta_score(Y_v, p_v_reconstructed, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94096011664754398"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_t_reconstructed = (np.mean(reconstruct(X_t), axis=0) > 0.2).astype(int)\n",
    "fbeta_score(Y_t, p_t_reconstructed, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94111441765341974"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_reconstructed = (np.mean(reconstruct(X), axis=0) > 0.2).astype(int)\n",
    "fbeta_score(Y, p_reconstructed, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 85)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 17)                1462      \n",
      "=================================================================\n",
      "Total params: 1,462\n",
      "Trainable params: 1,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import rmsprop\n",
    "inp = Input(batch_shape=(None, dim_classes*dim_models))\n",
    "x = Dense(17, activation='sigmoid')(inp)\n",
    "model2 = Model(inputs=inp, outputs=x)\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 259065 samples, validate on 64767 samples\n",
      "Epoch 1/100\n",
      "4s - loss: 0.1895 - acc: 0.9483 - val_loss: 0.1005 - val_acc: 0.9705\n",
      "Epoch 2/100\n",
      "4s - loss: 0.0890 - acc: 0.9717 - val_loss: 0.0814 - val_acc: 0.9726\n",
      "Epoch 3/100\n",
      "4s - loss: 0.0789 - acc: 0.9729 - val_loss: 0.0765 - val_acc: 0.9732\n",
      "Epoch 4/100\n",
      "4s - loss: 0.0758 - acc: 0.9733 - val_loss: 0.0747 - val_acc: 0.9735\n",
      "Epoch 5/100\n",
      "4s - loss: 0.0745 - acc: 0.9735 - val_loss: 0.0739 - val_acc: 0.9736\n",
      "Epoch 6/100\n",
      "4s - loss: 0.0738 - acc: 0.9737 - val_loss: 0.0733 - val_acc: 0.9738\n",
      "Epoch 7/100\n",
      "4s - loss: 0.0733 - acc: 0.9738 - val_loss: 0.0729 - val_acc: 0.9738\n",
      "Epoch 8/100\n",
      "4s - loss: 0.0729 - acc: 0.9739 - val_loss: 0.0726 - val_acc: 0.9740\n",
      "Epoch 9/100\n",
      "4s - loss: 0.0727 - acc: 0.9739 - val_loss: 0.0724 - val_acc: 0.9740\n",
      "Epoch 10/100\n",
      "4s - loss: 0.0725 - acc: 0.9740 - val_loss: 0.0723 - val_acc: 0.9740\n",
      "Epoch 11/100\n",
      "4s - loss: 0.0723 - acc: 0.9740 - val_loss: 0.0721 - val_acc: 0.9741\n",
      "Epoch 12/100\n",
      "4s - loss: 0.0722 - acc: 0.9741 - val_loss: 0.0720 - val_acc: 0.9741\n",
      "Epoch 13/100\n",
      "4s - loss: 0.0720 - acc: 0.9741 - val_loss: 0.0719 - val_acc: 0.9741\n",
      "Epoch 14/100\n",
      "4s - loss: 0.0720 - acc: 0.9741 - val_loss: 0.0718 - val_acc: 0.9742\n",
      "Epoch 15/100\n",
      "4s - loss: 0.0719 - acc: 0.9741 - val_loss: 0.0718 - val_acc: 0.9741\n",
      "Epoch 16/100\n",
      "4s - loss: 0.0718 - acc: 0.9741 - val_loss: 0.0717 - val_acc: 0.9742\n",
      "Epoch 17/100\n",
      "4s - loss: 0.0717 - acc: 0.9742 - val_loss: 0.0716 - val_acc: 0.9742\n",
      "Epoch 18/100\n",
      "4s - loss: 0.0717 - acc: 0.9742 - val_loss: 0.0716 - val_acc: 0.9743\n",
      "Epoch 19/100\n",
      "4s - loss: 0.0716 - acc: 0.9742 - val_loss: 0.0715 - val_acc: 0.9743\n",
      "Epoch 20/100\n",
      "4s - loss: 0.0716 - acc: 0.9742 - val_loss: 0.0715 - val_acc: 0.9743\n",
      "Epoch 21/100\n",
      "4s - loss: 0.0715 - acc: 0.9743 - val_loss: 0.0715 - val_acc: 0.9743\n",
      "Epoch 22/100\n",
      "4s - loss: 0.0715 - acc: 0.9743 - val_loss: 0.0714 - val_acc: 0.9743\n",
      "Epoch 23/100\n",
      "4s - loss: 0.0714 - acc: 0.9743 - val_loss: 0.0714 - val_acc: 0.9743\n",
      "Epoch 24/100\n",
      "4s - loss: 0.0714 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9743\n",
      "Epoch 25/100\n",
      "4s - loss: 0.0714 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9743\n",
      "Epoch 26/100\n",
      "4s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9743\n",
      "Epoch 27/100\n",
      "4s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9743\n",
      "Epoch 28/100\n",
      "4s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0712 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "4s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0712 - val_acc: 0.9743\n",
      "Epoch 30/100\n",
      "4s - loss: 0.0712 - acc: 0.9743 - val_loss: 0.0712 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "4s - loss: 0.0712 - acc: 0.9743 - val_loss: 0.0712 - val_acc: 0.9743\n",
      "Epoch 32/100\n",
      "4s - loss: 0.0712 - acc: 0.9743 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "4s - loss: 0.0712 - acc: 0.9743 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "4s - loss: 0.0712 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "4s - loss: 0.0711 - acc: 0.9743 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0711 - val_acc: 0.9743\n",
      "Epoch 39/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "4s - loss: 0.0711 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "3s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0710 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "4s - loss: 0.0710 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "3s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9745\n",
      "Epoch 51/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "3s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "3s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "3s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "3s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "4s - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "3s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 63/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0709 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "3s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "3s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "3s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 69/100\n",
      "3s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 72/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "4s - loss: 0.0708 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9745\n",
      "Epoch 80/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "3s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 84/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "3s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 87/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 90/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 92/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 93/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 94/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 95/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 97/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9745\n",
      "Epoch 99/100\n",
      "4s - loss: 0.0707 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "4s - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0707 - val_acc: 0.9744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdee818af10>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAD8CAYAAADQSqd1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdee8542090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "history_callback = model2.fit(X_t, Y_t, batch_size=batch_size, epochs=epochs, verbose=2, \\\n",
    "          validation_data=(X_v, Y_v))\n",
    "\n",
    "# print(\"Max score %f at epoch %d\" % (np.max(scores), np.argmax(scores)))\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_callback.history['loss'])\n",
    "plt.plot(history_callback.history['val_loss'])\n",
    "plt.legend(labels=['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94116320723963653"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_v_final = (model2.predict(X_v) > 0.2).astype(int)\n",
    "fbeta_score(Y_v, pred_v_final, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94502204382852439"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_final = (model2.predict(X_train) > 0.2).astype(int)\n",
    "fbeta_score(y_true_train, pred_train_final, beta=2, average='samples')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
